

### **Pytorch一个基于python的科学计算包**

服务场合：

1.替代numpy发挥GPU性能

2.提供了高度灵活性和效率的深度学习实验性平台

#### Tensors

tensor表示张量，是一种类似于array和matrices的数据结构。pytorch一般用tensors表示输入和输出，同时也表示模型参数。

##### **1.初始化矩阵**

```python
from __future__ import print_function
import torch
x = torch.Tensor(5, 3)  # 构造一个未初始化的5*3的矩阵
x = torch.rand(5, 3)  # 构造一个随机初始化的矩阵
y = torch.rand(5, 3)
```

##### 2.将两种同类型矩阵相加

```python
#此处 将两个同形矩阵相加有两种语法结构
x + y # 语法一
torch.add(x, y) # 语法二

```

##### 3.输出矩阵

```python
result = torch.Tensor(5, 3) # 语法一
torch.add(x, y, out=result) # 语法二
y.add_(x) # 将y与x相加
```

tip：任何可以改变tensor内容的操作都会在方法后加一个_

##### 4.切片操作

```python
x[:,1] #这一操作会输出x矩阵的第二列的所有值
```

#### Numpy桥

##### 1.tensor转换为numpy

```python
a = torch.ones(5)
b = a.numpy()
```

##### 2.修改numpy数组之后，与之相关联的tensor也会相应被修改

```
a.add_(1)
print(a)
print(b)
```

##### 3.将numpy的array转换为tensor

修改numpy也会相应修改关联的tensor

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)
```

##### 4.当cuda可用会进行GPU运算

```python
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
```

### Pytorch中的神经网络

pytorch中所有神经网络都来自于autograd包

##### 1.autograd：自动求导

autograd包提供tensor所有操作的自动求导方法。

autograd.Variable是包中最核心的类。包装一个tensor并几乎支持所有定义在其上的操作（除了部分会修改tensor自身的inplace函数）。完成运算可以调用.backward()自动计算所有梯度。



属性.data：访问原始tensor

属性.grad：variable的所有梯度

属性.creator：引用一个创建Variable的Function

example:手动和自动计算*y*=*2x*∙*e**x*+*x*2∙*e**x*导数

```python
def f(x):
    '''计算y'''
    y = x**2 * t.exp(x)
    return y

def gradf(x):
    '''手动求导函数'''
    dx = 2*x*t.exp(x) + x**2*t.exp(x)
    return dx


```

```python
x = t.randn(3,4, requires_grad = True)
y = f(x)
y
```

tensor([[ 0.0928,  0.1978,  0.6754,  0.8037],
        [ 0.9882,  0.3546,  0.2380,  0.0002],
        [ 0.2863,  0.0448,  0.1516,  2.9122]])

```python
y.backward(t.ones(y.size())) # gradient形状与y一致
x.grad
```

tensor([[-0.4146, -0.4610,  2.9016,  3.2831],
        [ 3.8102,  1.8614, -0.4536, -0.0244],
        [-0.4321,  0.5110, -0.4549,  8.6048]])

```python
# autograd的计算结果与利用公式手动计算的结果一致
gradf(x) 
```

tensor([[-0.4146, -0.4610,  2.9016,  3.2831],
        [ 3.8102,  1.8614, -0.4536, -0.0244],
        [-0.4321,  0.5110, -0.4549,  8.6048]])

------



```python
x = t.ones(1)
b = t.rand(1, requires_grad = True)
w = t.rand(1, requires_grad = True)
y = w * x # 等价于y=w.mul(x)
z = y + b # 等价于z=y.add(b)
```

在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作Function，每一个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播过程中**，autograd沿着这个图从当前变量（根节点z ）溯源，可以利用链式求导法则计算所有叶子节点的梯度**。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。

```python
x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad
```

False   True   True   True

变量的`requires_grad`**属性默认为False**，如果某一个节点requires_grad被设置为True，那么**所有依赖它的节点`requires_grad`都是True**。

```python
# 使用retain_graph来保存buffer
z.backward(retain_graph=True)
w.grad
```

计算w的梯度的时候，需要用到x的数值，这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定`retain_graph`来保留这些buffer。

```python
a = t.ones(3,4,requires_grad=True)
b = t.ones(3,4,requires_grad=True)
c = a * b

a.data # 还是一个tensor
```

tensor([[ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]])

```python
d = a.data.sigmoid_() 
# sigmoid_ 是个inplace操作，会修改a自身的值
a
```

tensor([[ 0.7311,  0.7311,  0.7311,  0.7311],
        [ 0.7311,  0.7311,  0.7311,  0.7311],
        [ 0.7311,  0.7311,  0.7311,  0.7311]])

如果我们想要修改tensor的数值，但是又不希望被autograd记录，那么我么可以**对tensor.data进行操作**

------

```python
from torch.autograd import Variable
x = Variable(torch.ones(2, 2), requires_grad = True)
y = x + 2
y.creator

# y 是作为一个操作的结果创建的因此y有一个creator 
z = y * y * 3
out = z.mean()

# 现在我们来使用反向传播
out.backward()

# out.backward()和操作out.backward(torch.Tensor([1.0]))是等价的
# 在此处输出 d(out)/dx
x.grad
```

最终得出的结果应该是一个全是4.5的矩阵。设置输出的变量为**o**。我们通过这一公式来计算：

![[公式]](https://www.zhihu.com/equation?tex=o+%3D+%5Cfrac%7B1%7D%7B4%7D%5Csum_i+z_i)，![[公式]](https://www.zhihu.com/equation?tex=z_i+%3D+3%28x_i%2B2%29%5E2)，![[公式]](https://www.zhihu.com/equation?tex=z_i%5Cbigr%5Crvert_%7Bx_i%3D1%7D+%3D+27)，因此，![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+o%7D%7B%5Cpartial+x_i%7D+%3D+%5Cfrac%7B3%7D%7B2%7D%28x_i%2B2%29)，最后有![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+o%7D%7B%5Cpartial+x_i%7D%5Cbigr%5Crvert_%7Bx_i%3D1%7D+%3D+%5Cfrac%7B9%7D%7B2%7D+%3D+4.5)



如果你想要进行求导计算，你可以在Variable上调用.backward()。 如果Variable是一个标量（例如它包含一个单元素数据），你无需对backward()指定任何参数，然而如果它有更多的元素，你需要指定一个和tensor的形状想匹配的grad_output参数。

##### 2.神经网络

使用torch.nn可以进行神经网络构建。

nn.Module包含神经网络的层，forward（input）方法能将output返回。

![preview](6.assets/v2-06a914f4ee93f25c0d6c924df9b4b4cb_r.jpg)

典型的神经网络训练过程：

- 定义一个有着可学习的参数（或者权重）的神经网络
- 对着一个输入的数据集进行迭代:
  - 用神经网络对输入进行处理
  - 计算代价值（对输入的修正）
  - 将梯度传播回神经网络参数中
  - 更新网络中的权重

