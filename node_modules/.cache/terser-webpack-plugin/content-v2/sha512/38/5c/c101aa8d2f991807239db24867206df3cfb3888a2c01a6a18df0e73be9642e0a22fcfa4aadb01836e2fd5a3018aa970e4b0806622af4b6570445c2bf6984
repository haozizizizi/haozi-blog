{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{403:function(t,s,a){\"use strict\";a.r(s);var n=a(54),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":t.$parent.slotKey}},[a(\"h3\",{attrs:{id:\"利用pytorch实现验证码识别\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#利用pytorch实现验证码识别\"}},[t._v(\"#\")]),t._v(\" 利用pytorch实现验证码识别\")]),t._v(\" \"),a(\"h4\",{attrs:{id:\"_1-生成验证码图片\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-生成验证码图片\"}},[t._v(\"#\")]),t._v(\" 1.生成验证码图片\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-python extra-class\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" captcha\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"image \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" ImageCaptcha\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" time\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" random\\ncaptcha_array\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"list\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"0123456789abcdefghijklmnopqrstuvwxyz\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\ncaptcha_size\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"4\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" __name__ \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'__main__'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" i \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"in\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"range\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n    image\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"ImageCaptcha\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#导入captcha包下的image文件中的ImageCaptcha类,使用之前先实例化\")]),t._v(\"\\n    image_text\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"join\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"random\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"sample\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"captcha_array\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"captcha_size\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#该方法在array中随机选取四个元素\")]),t._v(\"\\n    image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"./datasets/test/{}_{}.png\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"format\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"image_text\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"int\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"time\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"time\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#格式化函数，将图片命名规范化\")]),t._v(\"\\n    image\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"write\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"image_text\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])])]),a(\"h4\",{attrs:{id:\"_2-datasets数据加载\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-datasets数据加载\"}},[t._v(\"#\")]),t._v(\" 2.datasets数据加载\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-python extra-class\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" torch\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"utils\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"data \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" Dataset\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" PIL \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" Image\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" torchvision \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" transforms\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" torch\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"utils\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"tensorboard \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" SummaryWriter\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" os\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" one_hot\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"my_dataset\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"Dataset\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"def\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"__init__\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"root_dir\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"super\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"my_dataset\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"__init__\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n        self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"list_image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\" os\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"join\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"root_dir\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"image_name\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" image_name \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"in\")]),t._v(\" os\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"listdir\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"root_dir\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n        self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Compose\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n\\n            \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"\\n              transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"ToTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n              transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Resize\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"160\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n              transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Grayscale\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n            \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\\n\\n\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# print(self.image_path)\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"def\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"__len__\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"list_image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"__len__\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"def\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"__getitem__\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" index\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n        image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"list_image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"index\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n        image_\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"Image\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"open\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n        tesor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" self\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"transforms\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"image_\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# image.show()\")]),t._v(\"\\n        label\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"image_path\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"split\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n        label\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"label\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"split\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"_\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n        label_tensor\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"one_hot\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"text2Vec\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"label\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n        label_tensor\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"label_tensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"view\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" tesor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"label_tensor\\n        \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# print(image_path)\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" __name__ \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'__main__'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n    writer\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"SummaryWriter\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"logs\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    train_data\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"my_dataset\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"./datasets/train/\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    img\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"label\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"train_data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"print\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"img\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"shape\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"label\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"shape\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    writer\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"add_image\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"img\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"img\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    writer\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"close\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])])]),a(\"h4\",{attrs:{id:\"one-hot-编码\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#one-hot-编码\"}},[t._v(\"#\")]),t._v(\" one hot 编码\")]),t._v(\" \"),a(\"p\",[t._v(\"one hot编码是将类别变量转换为机器学习算法易于利用的一种形式的过程（将字母转化为tensor）此处one hot指生成4列36行的数据\")]),t._v(\" \"),a(\"h5\",{attrs:{id:\"文本转化为变量\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#文本转化为变量\"}},[t._v(\"#\")]),t._v(\" 文本转化为变量\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-python extra-class\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"def\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"text2vec\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"text\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n    vectors\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"torch\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"zeros\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"common\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"captcha_size\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"common\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"captcha_array\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"__len__\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# vectors[0,0] = 1\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# vectors[1,3] = 1\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# vectors[2,4] = 1\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# vectors[3, 1] = 1\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" i \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"in\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"range\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"len\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"text\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\":\")]),t._v(\"\\n                 vectors\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"i\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"common\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"captcha_array\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"index\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"text\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"i\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" vectors\\n\")])])]),a(\"h5\",{attrs:{id:\"变量还原为文本\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#变量还原为文本\"}},[t._v(\"#\")]),t._v(\" 变量还原为文本\")])])}),[],!1,null,null,null);s.default=e.exports}}]);","extractedComments":[]}